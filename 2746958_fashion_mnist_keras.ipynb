{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2746958_fashion_mnist_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datumScience/Fashion-MNIST/blob/master/2746958_fashion_mnist_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r2NPAI4jZZgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#On your own Assignment 3\n",
        "#Fashion MNIST\n",
        "\n",
        "![alt text](https://github.com/margaretmz/deep-learning/blob/master/images/modern%20dl_fash-mnist_keras.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "o2TXG9D7SZuT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Loading the dataset"
      ]
    },
    {
      "metadata": {
        "id": "d44TznbgZZgm",
        "colab_type": "code",
        "outputId": "210ef1d1-103c-4181-f605-d785f121ed71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow>=1.8.0\n",
        "import tensorflow as tf\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tWORMSC8FDR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize the data"
      ]
    },
    {
      "metadata": {
        "id": "aFe4wHGRFKle",
        "colab_type": "code",
        "outputId": "45d25f8e-6fcb-4253-aea5-d679f579b290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "cell_type": "code",
      "source": [
        "# Print training set shape - note there are 60,000 training data of image size of 28x28, 60,000 train labels)\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_test.shape[0], 'test set')\n",
        "\n",
        "# Define the text labels\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2 \n",
        "                        \"Dress\",        # index 3 \n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6 \n",
        "                        \"Sneaker\",      # index 7 \n",
        "                        \"Bag\",          # index 8 \n",
        "                        \"Ankle boot\"]   # index 9\n",
        "\n",
        "# Image index, you can pick any number between 0 and 59,999\n",
        "img_index = 5\n",
        "# y_train contains the lables, ranging from 0 to 9\n",
        "label_index = y_train[img_index]\n",
        "# Print the label, for example 2 Pullover\n",
        "print (\"y = \" + str(label_index) + \" \" +(fashion_mnist_labels[label_index]))\n",
        "# # Show one of the images from the training dataset\n",
        "plt.imshow(x_train[img_index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "60000 train set\n",
            "10000 test set\n",
            "y = 2 Pullover\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efdd676e630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Zx-Ee6LHZZgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data normalization\n",
        "Normalize the data dimensions so that they are of approximately the same scale."
      ]
    },
    {
      "metadata": {
        "id": "XNh5NIckZZgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMSg53fiZZgx",
        "colab_type": "code",
        "outputId": "1c889d61-1b50-49a3-e61f-3429300ed8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of train data - \" + str(len(x_train)))\n",
        "print(\"Number of test data - \" + str(len(x_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train data - 60000\n",
            "Number of test data - 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CFlNHktHBtru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split the data into train/validation/test data sets\n",
        "\n",
        "\n",
        "*   Training data - used for training the model\n",
        "*   Validation data - used for tuning the hyperparameters and evaluate the models\n",
        "*   Test data - used to test the model after the model has gone through initial vetting by the validation set.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1ShU787gZZg0",
        "colab_type": "code",
        "outputId": "9adea0e6-cf75-4d50-c38f-63576b8a8a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Further break training data into train / validation sets #performing a 80 - 20 split of data - 80% training and 20% validation\n",
        "(x_train, x_valid) = x_train[12000:], x_train[:12000] \n",
        "(y_train, y_valid) = y_train[12000:], y_train[:12000]\n",
        "\n",
        "# Reshape input data from (28, 28) to (28, 28, 1)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Print training set shape\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training, validation, and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (48000, 28, 28, 1) y_train shape: (48000, 10)\n",
            "48000 train set\n",
            "12000 validation set\n",
            "10000 test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xjW1-KhYPzzG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ]
    },
    {
      "metadata": {
        "id": "SbnEgLZpRbWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing necessary packages\n",
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXUIwkNuPzWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "num_classes = y_test.shape[1]\n",
        "#defining the baseline model as specified in the text book\n",
        "from keras import backend as K\n",
        "def baseline_model():\n",
        "# create model\n",
        "  K.set_image_dim_ordering( 'th' )\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "  # Compile model\n",
        "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "0071e3cb-1807-4c08-a44e-db0cbdd4dc5c",
        "id": "xc8_q3E6BlZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "baseline = baseline_model()\n",
        "# Fit the model\n",
        "baseline.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=10, batch_size=200,\n",
        "verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = baseline.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/10\n",
            " - 8s - loss: 0.4675 - acc: 0.8345 - val_loss: 0.3436 - val_acc: 0.8808\n",
            "Epoch 2/10\n",
            " - 7s - loss: 0.3142 - acc: 0.8875 - val_loss: 0.3164 - val_acc: 0.8861\n",
            "Epoch 3/10\n",
            " - 7s - loss: 0.2738 - acc: 0.9014 - val_loss: 0.2712 - val_acc: 0.9039\n",
            "Epoch 4/10\n",
            " - 7s - loss: 0.2444 - acc: 0.9112 - val_loss: 0.2618 - val_acc: 0.9091\n",
            "Epoch 5/10\n",
            " - 7s - loss: 0.2302 - acc: 0.9159 - val_loss: 0.2630 - val_acc: 0.9034\n",
            "Epoch 6/10\n",
            " - 7s - loss: 0.2095 - acc: 0.9245 - val_loss: 0.2540 - val_acc: 0.9078\n",
            "Epoch 7/10\n",
            " - 7s - loss: 0.1967 - acc: 0.9266 - val_loss: 0.2486 - val_acc: 0.9101\n",
            "Epoch 8/10\n",
            " - 7s - loss: 0.1788 - acc: 0.9345 - val_loss: 0.2414 - val_acc: 0.9150\n",
            "Epoch 9/10\n",
            " - 7s - loss: 0.1679 - acc: 0.9386 - val_loss: 0.2382 - val_acc: 0.9158\n",
            "Epoch 10/10\n",
            " - 7s - loss: 0.1519 - acc: 0.9437 - val_loss: 0.2462 - val_acc: 0.9131\n",
            "CNN Error: 9.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZR0RC75Rfd3",
        "colab_type": "code",
        "outputId": "60bec070-6570-4515-99a4-b87634152093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_accuracy(x_test,y_test,baseline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test accuracy: 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E1v-cQPAtfjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#New Model"
      ]
    },
    {
      "metadata": {
        "id": "QgTZ47SsZZg4",
        "colab_type": "code",
        "outputId": "f7ae9cce-27ea-4ddd-b2c7-f24338360559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Must define the input shape in the first layer of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Take a look at the model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 28, 28, 128)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 14, 14, 64)        32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 256)               803072    \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 878,346\n",
            "Trainable params: 878,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FhxJ5dinZZg8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compile the model\n",
        "Configure the learning process with compile() API before training the model. It receives three arguments:\n",
        "\n",
        "*   An optimizer \n",
        "*   A loss function \n",
        "*   A list of metrics \n"
      ]
    },
    {
      "metadata": {
        "id": "CQUlOa8cZZg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aTPUTIcGLmJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Must define the input shape in the first layer of the neural network\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  # Take a look at the model summary\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtOvh3YVZZg_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Now let's train the model with fit() API.\n",
        "\n",
        "We use  the [ModelCheckpoint](https://keras.io/callbacks/#modelcheckpoint) API to save the model after every epoch. Set \"save_best_only = True\" to save only when the validation accuracy improves.\n"
      ]
    },
    {
      "metadata": {
        "id": "GuEUmPUX8jXP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Defining the callback and ModelCheckpoint classes"
      ]
    },
    {
      "metadata": {
        "id": "P0-5LNYX8ogN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I kept getting an error while trying to invoke ModelCheckpoint defined in keras. (\"on_train_batch_begin\" attribute not found in ModelCheckpoint). To combat that, I copied the source code for ModelCheckpoint and its parent class Callback and added the outlines for on_train_batch_begin and on_train_batch_end."
      ]
    },
    {
      "metadata": {
        "id": "_XlTFFjS5iYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Callback(object):\n",
        "    \"\"\"Abstract base class used to build new callbacks.\n",
        "    # Properties\n",
        "        params: dict. Training parameters\n",
        "            (eg. verbosity, batch size, number of epochs...).\n",
        "        model: instance of `keras.models.Model`.\n",
        "            Reference of the model being trained.\n",
        "    The `logs` dictionary that callback methods\n",
        "    take as argument will contain keys for quantities relevant to\n",
        "    the current batch or epoch.\n",
        "    Currently, the `.fit()` method of the `Sequential` model class\n",
        "    will include the following quantities in the `logs` that\n",
        "    it passes to its callbacks:\n",
        "        on_epoch_end: logs include `acc` and `loss`, and\n",
        "            optionally include `val_loss`\n",
        "            (if validation is enabled in `fit`), and `val_acc`\n",
        "            (if validation and accuracy monitoring are enabled).\n",
        "        on_batch_begin: logs include `size`,\n",
        "            the number of samples in the current batch.\n",
        "        on_batch_end: logs include `loss`, and optionally `acc`\n",
        "            (if accuracy monitoring is enabled).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.validation_data = None\n",
        "        self.model = None\n",
        "\n",
        "    def set_params(self, params):\n",
        "        self.params = params\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        \"\"\"Called at the start of an epoch.\n",
        "        Subclasses should override for any actions to run. This function should only\n",
        "        be called during train mode.\n",
        "        # Arguments\n",
        "            epoch: integer, index of epoch.\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"Called at the end of an epoch.\n",
        "        Subclasses should override for any actions to run. This function should only\n",
        "        be called during train mode.\n",
        "        # Arguments\n",
        "            epoch: integer, index of epoch.\n",
        "            logs: dict, metric results for this training epoch, and for the\n",
        "                validation epoch if validation is performed. Validation result keys\n",
        "                are prefixed with `val_`.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        \"\"\"Called at the beginning of a training batch in `fit` methods.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dict, has keys `batch` and `size` representing the current\n",
        "                batch number and the size of the batch.\n",
        "        \"\"\"\n",
        "        # For backwards compatibility\n",
        "        self.on_batch_begin(batch, logs=logs)\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        \"\"\"Called at the end of a training batch in `fit` methods.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dict, metric results for this batch.\n",
        "        \"\"\"\n",
        "        # For backwards compatibility\n",
        "        self.on_batch_end(batch, logs=logs)\n",
        "\n",
        "    def on_test_batch_begin(self, batch, logs=None):\n",
        "        \"\"\"Called at the beginning of a batch in `evaluate` methods.\n",
        "        Also called at the beginning of a validation batch in the `fit` methods,\n",
        "        if validation data is provided.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dict, has keys `batch` and `size` representing the current\n",
        "                batch number and the size of the batch.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        \"\"\"Called at the end of a batch in `evaluate` methods.\n",
        "        Also called at the end of a validation batch in the `fit` methods,\n",
        "        if validation data is provided.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dict, metric results for this batch.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        \"\"\"Called at the beginning of a batch in `predict` methods.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dict, has keys `batch` and `size` representing the current\n",
        "                batch number and the size of the batch.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        \"\"\"Called at the end of a batch in `predict` methods.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dict, metric results for this batch.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        \"\"\"Called at the beginning of training.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        \"\"\"Called at the end of training.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_test_begin(self, logs=None):\n",
        "        \"\"\"Called at the beginning of evaluation or validation.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_test_end(self, logs=None):\n",
        "        \"\"\"Called at the end of evaluation or validation.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        \"\"\"Called at the beginning of prediction.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        \"\"\"Called at the end of prediction.\n",
        "        Subclasses should override for any actions to run.\n",
        "        # Arguments\n",
        "            logs: dict, currently no data is passed to this argument for this method\n",
        "                but that may change in the future.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch.\n",
        "    `filepath` can contain named formatting options,\n",
        "    which will be filled with the values of `epoch` and\n",
        "    keys in `logs` (passed in `on_epoch_end`).\n",
        "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
        "    then the model checkpoints will be saved with the epoch number and\n",
        "    the validation loss in the filename.\n",
        "    # Arguments\n",
        "        filepath: string, path to save the model file.\n",
        "        monitor: quantity to monitor.\n",
        "        verbose: verbosity mode, 0 or 1.\n",
        "        save_best_only: if `save_best_only=True`,\n",
        "            the latest best model according to\n",
        "            the quantity monitored will not be overwritten.\n",
        "        save_weights_only: if True, then only the model's weights will be\n",
        "            saved (`model.save_weights(filepath)`), else the full model\n",
        "            is saved (`model.save(filepath)`).\n",
        "        mode: one of {auto, min, max}.\n",
        "            If `save_best_only=True`, the decision\n",
        "            to overwrite the current save file is made\n",
        "            based on either the maximization or the\n",
        "            minimization of the monitored quantity. For `val_acc`,\n",
        "            this should be `max`, for `val_loss` this should\n",
        "            be `min`, etc. In `auto` mode, the direction is\n",
        "            automatically inferred from the name of the monitored quantity.\n",
        "        period: Interval (number of epochs) between checkpoints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n",
        "                 save_best_only=False, save_weights_only=False,\n",
        "                 mode='auto', period=1):\n",
        "        super(ModelCheckpoint, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.save_weights_only = save_weights_only\n",
        "        self.period = period\n",
        "        self.epochs_since_last_save = 0\n",
        "\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
        "                          'fallback to auto mode.' % (mode),\n",
        "                          RuntimeWarning)\n",
        "            mode = 'auto'\n",
        "\n",
        "        if mode == 'min':\n",
        "            self.monitor_op = np.less\n",
        "            self.best = np.Inf\n",
        "        elif mode == 'max':\n",
        "            self.monitor_op = np.greater\n",
        "            self.best = -np.Inf\n",
        "        else:\n",
        "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = -np.Inf\n",
        "            else:\n",
        "                self.monitor_op = np.less\n",
        "                self.best = np.Inf\n",
        "\n",
        "\n",
        "    def on_train_batch_begin(self,batch,logs={}):\n",
        "      return\n",
        "    def on_train_batch_end(self,batch,logs={}):\n",
        "      return\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save >= self.period:\n",
        "            self.epochs_since_last_save = 0\n",
        "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
        "            if self.save_best_only:\n",
        "                current = logs.get(self.monitor)\n",
        "                if current is None:\n",
        "                    warnings.warn('Can save best model only with %s available, '\n",
        "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
        "                else:\n",
        "                    if self.monitor_op(current, self.best):\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
        "                                  ' saving model to %s'\n",
        "                                  % (epoch + 1, self.monitor, self.best,\n",
        "                                     current, filepath))\n",
        "                        self.best = current\n",
        "                        if self.save_weights_only:\n",
        "                            self.model.save_weights(filepath, overwrite=True)\n",
        "                        else:\n",
        "                            self.model.save(filepath, overwrite=True)\n",
        "                    else:\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\n",
        "                                  (epoch + 1, self.monitor, self.best))\n",
        "            else:\n",
        "                if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
        "                if self.save_weights_only:\n",
        "                    self.model.save_weights(filepath, overwrite=True)\n",
        "                else:\n",
        "                    self.model.save(filepath, overwrite=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzNwlT099JHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Mounting gdrive"
      ]
    },
    {
      "metadata": {
        "id": "-KpCrINp4ib0",
        "colab_type": "code",
        "outputId": "9e35b0b5-ee06-4bd8-c1e7-942fe58b092c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "USfX3ESX9LhR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Saving model with best validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "viAelUpf6pzj",
        "colab_type": "code",
        "outputId": "380446c6-daf2-4703-aeac-79184cb914b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/myweights-improvement.hdf5', verbose = 1, save_best_only=True)\n",
        "model.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=64,\n",
        "         epochs=10,\n",
        "         validation_data=(x_valid, y_valid),\n",
        "         callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/10\n",
            "47872/48000 [============================>.] - ETA: 0s - loss: 0.9130 - acc: 0.6646\n",
            "Epoch 00001: val_loss improved from inf to 0.47138, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 16s 336us/sample - loss: 0.9121 - acc: 0.6649 - val_loss: 0.4714 - val_acc: 0.8155\n",
            "Epoch 2/10\n",
            "47936/48000 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.8074\n",
            "Epoch 00002: val_loss improved from 0.47138 to 0.38578, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 283us/sample - loss: 0.5560 - acc: 0.8074 - val_loss: 0.3858 - val_acc: 0.8598\n",
            "Epoch 3/10\n",
            "47936/48000 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.8369\n",
            "Epoch 00003: val_loss improved from 0.38578 to 0.33745, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 283us/sample - loss: 0.4792 - acc: 0.8369 - val_loss: 0.3374 - val_acc: 0.8767\n",
            "Epoch 4/10\n",
            "47744/48000 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8555\n",
            "Epoch 00004: val_loss improved from 0.33745 to 0.31540, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 284us/sample - loss: 0.4313 - acc: 0.8554 - val_loss: 0.3154 - val_acc: 0.8878\n",
            "Epoch 5/10\n",
            "47872/48000 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8657\n",
            "Epoch 00005: val_loss improved from 0.31540 to 0.29885, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 283us/sample - loss: 0.3985 - acc: 0.8658 - val_loss: 0.2988 - val_acc: 0.8913\n",
            "Epoch 6/10\n",
            "47872/48000 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8732\n",
            "Epoch 00006: val_loss improved from 0.29885 to 0.27635, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 286us/sample - loss: 0.3774 - acc: 0.8731 - val_loss: 0.2763 - val_acc: 0.9017\n",
            "Epoch 7/10\n",
            "47872/48000 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8773\n",
            "Epoch 00007: val_loss improved from 0.27635 to 0.26919, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 283us/sample - loss: 0.3631 - acc: 0.8773 - val_loss: 0.2692 - val_acc: 0.9035\n",
            "Epoch 8/10\n",
            "47808/48000 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8824\n",
            "Epoch 00008: val_loss improved from 0.26919 to 0.26066, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 289us/sample - loss: 0.3468 - acc: 0.8824 - val_loss: 0.2607 - val_acc: 0.9051\n",
            "Epoch 9/10\n",
            "47744/48000 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8837\n",
            "Epoch 00009: val_loss improved from 0.26066 to 0.25351, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 14s 291us/sample - loss: 0.3392 - acc: 0.8840 - val_loss: 0.2535 - val_acc: 0.9050\n",
            "Epoch 10/10\n",
            "47872/48000 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8890\n",
            "Epoch 00010: val_loss improved from 0.25351 to 0.25022, saving model to /content/gdrive/My Drive/myweights-improvement.hdf5\n",
            "48000/48000 [==============================] - 13s 280us/sample - loss: 0.3268 - acc: 0.8890 - val_loss: 0.2502 - val_acc: 0.9084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efdd5dbc240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "metadata": {
        "id": "c7dkF9oz8TVB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Load model with best validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "3wj3CNPA8J56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the weights with the best validation accuracy\n",
        "model.load_weights('/content/gdrive/My Drive/myweights-improvement.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZVh9E2na9c3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Test Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "2s0LAUBj9ccf",
        "colab_type": "code",
        "outputId": "4dd2e04b-77eb-4718-adb4-8773e135d6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print test accuracy\n",
        "print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test accuracy: 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9RTRkan4yq5H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "VZtqBqFFy62R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_accuracy(x_test,y_test,model):\n",
        "  # Evaluate the model on test set\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "  # Print test accuracy\n",
        "  print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRxTWPuNpv72",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "TBbJlthJWjrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Feature Standardization"
      ]
    },
    {
      "metadata": {
        "id": "mPc1V2xxWren",
        "colab_type": "code",
        "outputId": "31f5693d-a94f-4bd9-9227-83605632d077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1202
        }
      },
      "cell_type": "code",
      "source": [
        "# Standardize images across the dataset, mean=0, stdev=1\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/myweights-improvement-fs.hdf5', verbose = 1, save_best_only=True)\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28,1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28,1)\n",
        "# convert from int to float\n",
        "x_train = x_train.astype( 'float32' )\n",
        "x_test = x_test.astype( 'float32' )\n",
        "# define data preparation\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train)\n",
        "#creating validation generator\n",
        "val_datagen = ImageDataGenerator(zca_whitening=True)\n",
        "val_datagen.fit(x_valid)\n",
        "\n",
        "\n",
        "model = cnn_model()\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),steps_per_epoch=len(x_train) / 32, epochs=10,validation_data = val_datagen.flow(x_valid, y_valid, batch_size=32),callbacks=[checkpointer])\n",
        "\n",
        "# configure batch size and retrieve one batch of images\n",
        "for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
        "# create a grid of 3x3 images\n",
        "  for i in range(0, 9):\n",
        "   # model.fit(x_batch, y_batch )\n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap( 'gray' ))\n",
        "# show the plot\n",
        "  pyplot.show()\n",
        "  break\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:923: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (48000, 28, 28, 1) (28 channels).\n",
            "  ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:334: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:923: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (12000, 28, 28, 1) (28 channels).\n",
            "  ' channels).')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_20 (Conv2D)           (None, 28, 28, 128)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 14, 14, 64)        32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 256)               803072    \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 878,346\n",
            "Trainable params: 878,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3, or 4 channels on axis 1. However, it was passed an array with shape (48000, 28, 28, 1) (28 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3, or 4 channels on axis 1. However, it was passed an array with shape (12000, 28, 28, 1) (28 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 5s 13ms/step - loss: 1.1609 - acc: 0.6304\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16086, saving model to /content/gdrive/My Drive/myweights-improvement-fs.hdf5\n",
            "1500/1500 [==============================] - 30s 20ms/step - loss: 0.8577 - acc: 0.6855 - val_loss: 1.1609 - val_acc: 0.6304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAFMCAYAAABBKVJLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6FFnJoQQIvT8H71jWxy42rf/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QSfD9dtfES0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracy with feature standardization is quite less, giving only 28% accuracy with the validation set. "
      ]
    },
    {
      "metadata": {
        "id": "ppUVhfnIOEQC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the weights with the best validation accuracy\n",
        "model.load_weights('/content/gdrive/My Drive/myweights-improvement-fs.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwvDC4qkEgOv",
        "colab_type": "code",
        "outputId": "f61d25ff-9f4a-478a-d950-96e79a78e024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "  test_accuracy(x_test,y_test,model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test accuracy: 0.408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aLl9aDKPF385",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After choosing the best epoch and evaluating the test set performance we get 91.63% accuracy."
      ]
    },
    {
      "metadata": {
        "id": "lQ_oaiD4Xcrs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##ZCA Whitening"
      ]
    },
    {
      "metadata": {
        "id": "63asRsA3Xe0_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ZCA whitening\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "# load data\n",
        "checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/myweights-improvement-zca.hdf5', verbose = 1, save_best_only=True)\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28,1)\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0],28, 28,1)\n",
        "# convert from int to float\n",
        "x_train = x_train.astype( 'float32' )\n",
        "x_test = x_test.astype( 'float32' )\n",
        "# define data preparation\n",
        "datagen = ImageDataGenerator(zca_whitening=True)\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train)\n",
        "\n",
        "#creating validation generator\n",
        "val_datagen = ImageDataGenerator(zca_whitening=True)\n",
        "val_datagen.fit(x_valid)\n",
        "\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),steps_per_epoch=len(x_train) / 32, epochs=10,validation_data = val_datagen.flow(x_valid, y_valid, batch_size=32),callbacks=[checkpointer])\n",
        "\n",
        "# configure batch size and retrieve one batch of images\n",
        "for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
        "# create a grid of 3x3 images\n",
        "  for i in range(0, 9):\n",
        "    #model.fit(x_batch,y_batch)\n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap( 'gray' ))\n",
        "# show the plot\n",
        "  pyplot.show()\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CejM9xZ8ISVH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracy on the validation set is 87.79%"
      ]
    },
    {
      "metadata": {
        "id": "hcvc7j3YOdlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the weights with the best validation accuracy\n",
        "model.load_weights('/content/gdrive/My Drive/myweights-improvement-zca.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFYohFgqGqpR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_accuracy(x_test,y_test,model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQP8EEUzIL6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracy is relatively better with ZCA whitening at 58.2%"
      ]
    },
    {
      "metadata": {
        "id": "76wvV0cNduvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Random Shifts"
      ]
    },
    {
      "metadata": {
        "id": "5Vx8IQHqduYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Random Rotations\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/myweights-improvement-rs.hdf5', verbose = 1, save_best_only=True)\n",
        "x_train = x_train.reshape(x_train.shape[0],  28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],  28, 28 ,1)\n",
        "# convert from int to float\n",
        "x_train = x_train.astype( 'float32' )\n",
        "x_test = x_test.astype( 'float32' )\n",
        "# define data preparation\n",
        "datagen = ImageDataGenerator(rotation_range=90)\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train)\n",
        "\n",
        "\n",
        "#creating validation generator\n",
        "val_datagen = ImageDataGenerator(zca_whitening=True)\n",
        "val_datagen.fit(x_valid)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),steps_per_epoch=len(x_train) / 32, epochs=10,validation_data =  val_datagen.flow(x_valid, y_valid, batch_size=32),callbacks=[checkpointer])\n",
        "\n",
        "# configure batch size and retrieve one batch of images\n",
        "for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
        "# create a grid of 3x3 images\n",
        "  for i in range(0, 9):\n",
        "\n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap( 'gray' ))\n",
        "    # show the plot\n",
        "  pyplot.show()\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JubCOoVtOmDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the weights with the best validation accuracy\n",
        "model.load_weights('/content/gdrive/My Drive/myweights-improvement-rs.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHyhAS-TGs5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_accuracy(x_test,y_test,model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sX2-2hVILruW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Random shift performs even better giving an accuracy  of 78.62%"
      ]
    },
    {
      "metadata": {
        "id": "qm-42M8LdHvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Random Flips"
      ]
    },
    {
      "metadata": {
        "id": "t_IB3HwfdJfv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Random Flips\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "# load data\n",
        "checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/myweights-improvement-rf.hdf5', verbose = 1, save_best_only=True)\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "x_train = x_train.reshape(x_train.shape[0],  28, 28,1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28,1)\n",
        "# convert from int to float\n",
        "x_train = x_train.astype( 'float32' )\n",
        "x_test = x_test.astype( 'float32' )\n",
        "# define data preparation\n",
        "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train)\n",
        "\n",
        "\n",
        "#creating validation generator\n",
        "val_datagen = ImageDataGenerator(zca_whitening=True)\n",
        "val_datagen.fit(x_valid)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),steps_per_epoch=len(x_train) / 32, epochs=10,validation_data = val_datagen.flow(x_valid, y_valid, batch_size=32),callbacks=[checkpointer])\n",
        "\n",
        "# configure batch size and retrieve one batch of images\n",
        "for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
        "# create a grid of 3x3 images\n",
        "  for i in range(0, 9):\n",
        "\n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap( 'gray' ))\n",
        "  # show the plot\n",
        "  pyplot.show()\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OM52xiA5GuzU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_accuracy(x_test,y_test,model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CPdUnEoMn3T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have a even higher accuracy of 82.37% with the random flips."
      ]
    },
    {
      "metadata": {
        "id": "qk6E2q00a8Hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Saving to file"
      ]
    },
    {
      "metadata": {
        "id": "epxQGIDsa-QP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save augmented images to file\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering( 'th' )\n",
        "\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
        "# convert from int to float\n",
        "x_train = x_train.astype( 'float32' )\n",
        "x_test = x_test.astype( 'float32' )\n",
        "# define data preparation\n",
        "datagen = ImageDataGenerator()\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train)\n",
        "# configure batch size and retrieve one batch of images\n",
        "os.makedirs( '/content/gdrive/My Drive/augmented_images' )\n",
        "for X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9, save_to_dir= '/content/gdrive/My Drive/augmented_images' ,\n",
        "save_prefix= 'aug' , save_format= 'png' ):\n",
        "# create a grid of 3x3 images\n",
        "  for i in range(0, 9):\n",
        "    pyplot.subplot(330 + 1 + i)\n",
        "    pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap( 'gray' ))\n",
        "# show the plot\n",
        "  pyplot.show()\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJv7XEk10bOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize prediction\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the groud truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "metadata": {
        "id": "QwNmlfIC0YxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predictions with the Random flips pre processing technique. This model has the highest accuracy of the lot\n",
        "y_hat = model.predict(x_test)\n",
        "\n",
        "# Plot a random sample of 10 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 8))\n",
        "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(x_test[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = np.argmax(y_test[index])\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n",
        "                                  fashion_mnist_labels[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8AehWdRAVKN5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this exercise the baseline model seems to be performing best. The next steps could be to modify the network architecture of the model under consideration and then incorporating data preprocessing techniques to generate a more powerful CNN model that can effectively classify the items. Of the lot , we find that random flips are the most efficient, relative to other preprocessing techniques. "
      ]
    }
  ]
}